\input{template.tex}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\newcounter{Chapcounter}
%\newcommand\showmycounter{\addtocounter{Chapcounter}{1}\themycounter}
\newcommand{\lecture}[1] 
{ {\newpage
  \phantomsection %This is too make sure that the toc points to the correct page. Make sure the this doens't break.
  \newpage
  \addtocounter{Chapcounter}{1} \Large \underline{\textbf{ \color{Sepia} Lecture \theChapcounter:}} }   
  \addcontentsline{toc}{section}{ \color{Sepia} Lecture:~\theChapcounter~~#1}    
}
\usepackage{eso-pic}


\newlist{Properties}{enumerate}{2}
\setlist[Properties]{label=Property \arabic*.,itemindent=*}


%\pagecolor[rgb]{0.98, 0.96, 0.89}

%\usepackage[pages=some]{background}

%\backgroundsetup{
%scale=1,
%color=black,
%opacity=0.4,
%angle=0,
%contents={%
%  \includegraphics[width=\paperwidth,height=\paperheight]{scroll-background}
%  }%
%}

\pagecolor[rgb]{0.94,0.86,0.69} %Trying to get that old paper look. 
\color[rgb]{0.2,0.2,0.2} %Trying to get a more faded text look.

\newcommand{\verteq}{\rotatebox{90}{$\,=$}}
\newcommand{\equalto}[2]{\underset{\scriptstyle\overset{\mkern4mu\verteq}{#2}}{#1}}


\author{Rindra Razafy, ``Hagamena''}
\title{Borcherd Algebraic Geometry 1}
\setcounter{section}{-1}
\begin{document}
\maketitle

\tableofcontents

\newpage

\section{Prologue}
Some quick notes summarising the ``Algebraic geometry 1'' video lectures from R.E. Borcherds, found \href{https://youtube.com/playlist?list=PL8yHsr3EFj53j51FG6wCbQKjBgpjKa5PX}{here}.


\lecture{}
\section{Introduction}
\subsection{Examples}
\subsubsection{Pythagorean triangles}
\textbf{Problem:} How do we classify all Pythagorean triangles.

We will look at two ways of solving this:\begin{enumerate}
    \item \textbf{Algebraic way:} 
    We want to solve\begin{equation}
        x^2+y^2 = z^2 \text{ with }x,y,z\text{ coprime integers}
    \end{equation}
    
    If we look at the equation mod $4$ we notice that $x^2,y^2,z^2 \equiv 0,1 \mod 4$, since the squares mod $4$ all take these forms.
    
    So $z$ is odd and WLOG we assume that $x$ is even and $y$ is odd. 
    We rearange the equation:\begin{equation}
        y^2 = z^2-x^2 = (z-x)(z+x)
    \end{equation}
    Assume that $z-x = dm_1$ and $z+x = dm_2$, therefore we have that $2z = d(m_1+m_2)$ and $2x = d(m_2-m_1)$, then since $d\mid 2z$ and $d\mid 2x$, and $\gcd(x,z) = 1$ we have two cases, either $d$ divides both $x$ and $z$, which would imply that $d=1$.
    
    Or $d$ divides $2$ which means that $d = 1$, or $d=2$. But note that since $x,z$ are of opposite parity $z+x$ is odd so $d\neq 2$.
    
    So in all cases, $d = 1$. So $(z-x)$ and $(z+x)$ are coprime.
    
    But since their product is a square this implies that $z-x$ and $z+x$ are squares, so:\begin{equation}
        z-x = r^2, \text{ and }z+x = s^2, \text{ where }s,r\text{ are odd and coprime}
    \end{equation}
    
    So we conclude that $z = \frac{r^2+s^2}{2}$, $x = \frac{s^2-r^2}{2}, y=rs $ for any $r,s$ odd and coprime.
    
    \item \textbf{Geometric solution}
    Let $X = \frac{x}{z}$, $Y = \frac{y}{z}$ and we want to solve\begin{equation}
        X^2+Y^2=1, \ X,Y\text{rational}
    \end{equation}
    
    So we are looking for rational points on the unit circle.
    
    Note if we draw the line from $(-1,0)$ to $(X,Y)$ on the unit circle with $X,T\in \Q$.It will intersect the y-axis at the point $(0,t)$ where $t=\frac{Y}{X+1}\in \Q$.
    
    Conversely, if we are given $t$ we can find $(X,Y)$, since we know that\begin{equation*}
        Y=t(X+1) \text{ and }t^2{(X+1)}^2 + X^2 = 1 \Rightarrow (X+1)((t^2+1)X+t^2-1) = 0
    \end{equation*}
    And finding roots we see that $X = \frac{1-t^2}{1+t^2}$ and $Y=\frac{2t}{1+t^2}$, for $t\in \Q$.
    
    \
    
    So there is a correspondence between points on the circle except for the point at $(-1,0)$ and points on the $y-$axis. This is what is called a Birational Equivalence.
    
    \begin{definition}
        \textbf{Birational Equivalence}
        An equivalence excepts on subsets of co-dimension at least $1$.
    \end{definition}
\end{enumerate}
Treating this problem as a geometrical problem gives us additional insights. Indeed, for example the circle forms a group of rotations with operation:\begin{equation}
    (x_1,y_1)\times (x_2,y_2) = (x_1x_2-y_1y_2,x_1y_2+x_2y_1)
\end{equation}

This is the cosine and sign of the sum of two angles, indeed if $(x_1,y_1) = (\cos\theta_1,\sin\theta_1)\text{ and }(x_2,y_2) = (\cos\theta_2,\sin\theta_2)$ then:\begin{equation}
 (\cos\theta_1,\sin\theta_1)\times (\cos\theta_2,\sin\theta_2) = (\cos\theta_1\cos\theta_2-\sin\theta_1\sin\theta_2, \dots) = (\cos(\theta_1+\theta_2),\sin(\theta_1+\theta_2))
\end{equation}
This is the simplest example of what is called an Algebraic group.

\begin{definition}
    \textbf{Algebraic Groups} We can think of this as functor from (commutative) Rings to Groups.

    \begin{equation}
        G\colon R\rightarrow (\{(x,y)\in R^2\mid x^2+y^2=1\},\times)
     \end{equation}
     Where the operation is defined as above, and the identity is $(1,0)$ and ${(x,y)}^{-1} = (x,-y)$.
     
\end{definition}

\begin{example}
    $G(\C) = \{(x,y)\in\C \mid x^2+y^2 = 1\}$
\end{example} 

But note that $1 = x^2+y^2 = {\underbrace{(x+iy)}_z}{\underbrace{(x-iy)}_{\overline{z}}}$. So we see that \begin{equation}
    G(\C) = \{(x,y)\in\C \mid x^2+y^2 = 1\}\simeq \{z\in \C \mid z\text{ is invertible}\} = \C^\ast
\end{equation}

\textbf{Summary}
There are many ways to view a circle:\begin{enumerate}
    \item Subset of $\R^2$
    \item Polynomial $x^2+y^2-1 \rightarrow$   Algebraic set
    \item Ideal $(x^2+y^2-1)$ in ring $\R[x,y]$.
    \item Ring $\R[x,y]/(x^2+y^2-1) = $ coordinate ring of $S^1$. Can be seen as the set of polynomials on the circle.
    \item (Smooth) manifold
    \item Group (Algebraic Group)
    \item Functor from Rings to Groups or Sets (Grothendieck)
\end{enumerate}

\section{Two cubic curves}
In this section we will discuss some cubic cubes.\begin{enumerate}
    \item $y^2 = x^3+x^2$
    
There is almost a 1-to-1 correspondence between $(x,y)$ rational on this curve and $t\in \Q$, via $t = \frac{y}{x}$, the slope of the line through $(x,y)$ and the origin. 
Indeed since $y=tx$, if $x\neq 0$, we have:\begin{equation}
    t^2x^2 = x^3+x^2 \Rightarrow t^2 = 1+x \Rightarrow x=t^2-1 \text{ and } y=t^3-t 
\end{equation}

We don't quite get a 1--1 correspondence because $t=1$ and $t=-1$ both correspond to $(x,y) = (0,0)$. 

So we can think of this cubic curve as a copy of $\Q$, but two of these points are mapped to the same point.

\begin{definition}
    Resolution of Singluarity
    A singularity is a ``bad'' point of our curve, and a resolution is getting a ``nice'' map from a curve without singularities to our curve.    
\end{definition} 

\

The resolution in the above case is done by a process called ``blowing-up''.
\begin{remark}
    Hironaka, showed that blowing-up resolves singularites in zero characteristic. (The problem in non-zero characteristic is still unsolved).
\end{remark}
\begin{remark}
    Finding rational points on curves can be difficult. For example:\begin{equation}
        x^n + y^n = 1 \Rightarrow X^n+Y^n=Z^n \text{ where }x=X/Z \text{ and }y=Y/Z
    \end{equation}
    This is Fermat's Last Theorem, which was very hard to solve.        
\end{remark}

\item $x^3+y^3 = 9$

Note on this curve we can define an algebraic operation ``+'', if we add in a point at infinity.
In that case, the point at infinity is the identity ``0'', and $a,b,c$ on the curve lie on a line if and only if $a+b+c = 0$ in the group. To check that the group operation is associative we use the fact that: $a_1+a_2+\cdots = b_1+b_2+\dots \iff $there is a rational function with poles at $a_i$ and zeroes at the $b_i$.

\begin{definition}
    Groups of this kind are called \textbf{elliptic curves}, there are the 1-dimensional case of what is called \textbf{Abelian varieties}. 
    Abelian varieties are algebraic groups that are ``projective'', roughly they have no missing points.
\end{definition} 

\end{enumerate}

\section{Bézout, Pappus, Pascal}
\subsection{Bézout's theorem}
\begin{theorem} \textbf{Bézout}
    Informally: Two curves of degree $m,n$ in the plane have at most $mn$ intersection, if they have no components in common.    

    \textbf{Stronger version of Bézout}
    There have exactly $mn$ intersection points if:\begin{enumerate}
        \item We are working over $\C$
        \item Count points at infinity
        \item Counting multiplicities (for example a straight line tangent to a parabola, we need to count the intersection point as two points).
    \end{enumerate}
\end{theorem}
This theorem was originally stated by Newton, though he didn't really prove it.

\

It is actually quite difficult to make sense of multiplicities.

\begin{proof}
    Informal proof: Suppose the curves are $f(x,y) = 0$ of degree $m$ and $g(x,y) = 0$ of degree $n$.

    Perturb $f,g$ so that $f = p_1\dots p_m$ and $g = q_1\dots q_n$, with $p_i.q_j$ linear.

    Problem: How do we know the number of intersection points doesn't change as we perturb $f,g$
\end{proof}

This style of proof was very common in the Italian School of Algebraic Geometry, but with these informal reasonings caused them to introduce many false theorems.

Weil and Zariski put Algebraic Geometry on much firmer foundations, but the proofs became much more complicated. 

Nowadays, these informal proofs are mostly useful just to guess what the right answer is (for example understanding the reasoning for the above proof, we can understand the reasoning for 
the anologue of Bézout's theomre in higher dimensions).


\subsection{Pappus' theorem}

\begin{theorem}
    Informally: Take two straight lines in the plane and on each line choose any three points.
Number them and join them to every point of a different number on the other line, looking at the intersection point of these lines.
These three intersection points lie on a straight line.
\end{theorem}

This theorem is equivalent to commutativity in multiplication. If we look at the anologuous result in a plane over a division ring then:\begin{equation*}
    \text{Pappus' theorem is true }\iff \text{ The division ring is a field (so multiplication commutes)}
\end{equation*}

\subsection{Pascal's theorem}
\begin{theorem}
    Informally: Choose any six points on an ellipse, seperate your ellipse into two, so that three points lie on one side and three points lie on the other side. Number the points on the first side as 1,2,3 and likewise for the points on the other side. 
    then join them to every point of a different number on the opposite side, looking at the intersection point of these lines.
    These three intersection points lie on a straight line.
\end{theorem}

    \href{https://youtu.be/-9qugwEZDJs?t=955}{\includegraphics[scale = 0.25]{pascal_theorem_pic}}

    \

    \textit{Illustration from the lecture video}

    \

The line on which their intersection lies is called the \textbf{Pascal line}.

\

Pappus' theorem is a degenerate case of Pascal's theorem, Pascal's theorm holds for any degree two curve and two straight lines are a degenerate case of a degree 2 curve.

How do we prove Pascal's theorem? We will use a proof using Algebraic Geometry and Bézout's theorem. 

\begin{proof}
    We number the lines as in the picture above (noting that they form a funny kind of hexagon) and choose six linear polynomials, 
    $p_i$, for $i\in \{1,\dots, 6\}$ where $p_i = 0$ on line $i$.

    Now look at $p_1p_3p_5$ and $p_2p_4p_6$, these polynomials vanish on all six points, so choose $\lambda$ such that:\begin{equation}
        p_1p_3p_5 - \lambda p_2p_4p_6 \text{ this is of degree 3 curve}
    \end{equation}
    Vanishes on a seventh point of the conic. Since the conic is of degree $2$, by Bézout there are at most $6$ intersection points 
    UNLESS they have a common component. So the conic must be contained in the degree $3$ curve.

    So this degree $3$ curve is equal to the union of a conic and a line, which is Pascal's line. Indeed since $p_1p_3p_5$ and $p_2p_4p_6$ both vanish on the 
    three intersection points, since they are on the curve but not on the conic, they must be on the line.
\end{proof}

\section{Kakeya sets}

We will continue to look at examples from Algebraic Geometry. Kakeya Sets are constructs from real analysis.

\begin{definition}
    The first definition of a \textbf{Kakeya set} is a set such that if you have a unit line in the set we can turn the line around
    in the set, for e.g. A circle, or an equilateral triange.

    Slight variation of the defintion, is that it is a set contianing a unit line in every direction.
\end{definition}

    A Kakeya set over a finite field F is a set that contains a line in every direction. A conjecture from T.Wolff:

    \begin{center}
    The size  of a Kakeya set over $F$ in $F^n$ is at least $c_n|F|^n$.
\end{center}
    
    This was proved in 2008 by Dvir with $c_n = \frac{1}{n!}$.
    
    The proof is in two steps:\begin{enumerate}
        \item  A Kakeya Set in $F^n$ cannot lie in a hypersurface of degree $d<|F|$.
        \begin{proof}
            Suppose $f$ is a polynomial of degree $d<|F|$ defining a hypersurface which is a Kakeya Set, and let $f_d$ be the highest degree component.
            Note that for any $v$ we can find $x$ so that $f(x+vt)$ vanishes for all $t$. This is what is meant by the zeroes of $f$ are a Kakeya set.
            For any direction $v$ we can find a line such that $f$ vanishes on that line.

            \

            So coefficient $f_d(v)$ of $t^d$ vanishes for any $v$, so $f_d$ has degree < $|F|$, since if $f_d$ was non-zero it would have at most $< |F|$ zeroes
            we must have that $f_d = 0$. So $f = 0$.
        \end{proof} 

        \item Observe, the polynomials of degree at most $|F|-1$ form a vector space of dimension $\binom{n+|F|-1}{n}$.

        So we can find hypersurface of degree at most $|F|-1$ vanishing on any set with less than $\binom{n+|F|-1}{n}$ points.
    \end{enumerate}

So a Kakeya set has at least $\binom{n+|F|-1}{n}$ points, but \begin{equation*}
    \binom{n+|F|-1}{n} = \frac{|F|(|F|+1)\cdots (|F|+n-1)}{1\cdot 2 \cdots n} \geq \frac{|F|^n}{n!}
\end{equation*}
    
\begin{example}
    27 lines on a cubic surface.

We will prove that the cubic surface:\begin{equation*}
    w^3+x^3+y^3+z^3 = 0 \text{ in }\mathbb{P}^3
\end{equation*}
Has exactly 27 lines on it.

\

Note $(w\colon x\colon y\colon z)\in \mathbb{P}^3$ then $(w\colon x\colon y\colon z) = (\lambda w\colon \lambda x\colon \lambda y\colon \lambda z)$, for all $\lambda\neq 0$.


Note there is an obvious line:\begin{equation*}
    (a\colon -a \colon b \colon -b) \text{ since }a^3+(-a)^3+b^3+(-b)^3 = 0
\end{equation*}

Note we can permute the coordinates and we can multiply by $\omega$ such that $\omega^3 = 1$.

This gives us $3\times 3 \times 3 = 27$ possibilities.

\end{example} 

\lecture{}
\section{Affine space and Zariski topology}

\subsection{Affine space}

\begin{definition}
    Let $k$ be any field (most commononly taken as $\C$, $\R$ or a finite field), then an \textbf{Affine space} is just $k^n$ as a vector space, with slightly different automorphism group.
    \begin{itemize}
        \item automorphism of a vector space is:\begin{equation*}
            GL_n(k) = \{n\times n\text{ matrices such that }\det \neq 0\}
        \end{equation*} 
        \item automorphism of Affine space is:\begin{equation*}
            GL_n(k) \text{ and } \{\text{translations, i.e a map }x\rightarrow x+v\text{ for some vector }v\}
        \end{equation*}

        If $n=2$ the group can be pictured of as the group of matrices of the following shape:
        \[\begin{bmatrix}
            \ast_1 & \ast_1 & \ast_2\\
            \ast_1 & \ast_1 & \ast_2\\
            0 & 0 & 1
        \end{bmatrix}\]

        Where $\begin{bmatrix}
            \ast_1 & \ast_1 \\
            \ast_1 & \ast_1 
        \end{bmatrix}\in GL_n(k)$ and $\begin{bmatrix}
             \ast_2\\
             \ast_2
        \end{bmatrix}$ corresponds to a translation.
    \end{itemize}
    We write an affine space as $\mathbb{A}^n$.

    Roughly speaking an affine space is a vector space where we have ``forgotten'' what our origin is.

    If we have a vector space we can get an affine space by ``forgotting'' $0$, and if we have an affine space we can choose any point to be the origin and it gives us a vector space.
\end{definition}

\begin{example}
    Note that if we look at the universe, the $3D$ space we live in is an affine space as there is no natural way to choose the origin.
    But if we choose the origin to be the center of the earth then $3D$ space becomes a vector space.
\end{example}

\subsubsection{Affine geometry}
\begin{definition}
    \textbf{Affine geometry} can be thought as the study of affine space that is invariant under translations and linear transformations.
\end{definition}

\paragraph*{Properties of affine geometry}

\begin{itemize}
    \item points
    \item lines
    \item parallel lines
    \item conics
    \item Polynomial functions
\end{itemize}

Are all well-defined in affine geometry.

\paragraph*{Not affine geometry}
\begin{itemize}
    \item circles
    \item angles
    \item lengths
\end{itemize}

\paragraph*{Coordinate ring of $\mathbb{A}^n$}
Algebraic geometry tends to use the coordinate ring of affine space.\begin{definition}
    The coordinate ring of $\mathbb{A}^n$, is just the space of polynomials on $\mathbb{A}^n$. Where $k$ is infinite.
    If we got affine space we can reconstruct the ring of polynomials on it. Conversely if we are given a polynomial ring over $k$ we can reconstruct affine space as:\begin{align*}
        \mathbb{A}^n &= \{\text{homomorphism from }k[x_1,\dots,x_n] \rightarrow k \text{ (as a k-algebra)} \}\\ 
    \end{align*}
    
    Indeed a homomorphism taking $k[x_1,\dots,x_n]\rightarrow k$, just takes $x_i\rightarrow a_i$ for some $a_i\in k$. This corresponds to the point $(a_1,\dots,a_n)\in \mathbb{A}^n$.    
\end{definition}

Because of this the study of affine space is more or less equivalent to the study of this polynomial ring. In particular the automorphism group of these two are the same.

\subsection{Zariski topology}
\begin{definition}
    An \textbf{algebraic set} is a set of zeros of some set of polynomials in $k[x_1,\dots,x_n]$. 
\end{definition}

\begin{example}
    If $f(x,y) = x^2+y^2-1$, then our algebric set is the circle. 
\end{example}
\begin{example}
    If $f(x) = x-a$ and $g(y) = y-b$, the our algebraic set is the point $(a,b)$.
\end{example}

Algebraic sets are closed under these operations:\begin{itemize}
    \item Intersection: Indeed if $C_1,C_2,\dots$ are the zero sets of $P_1,P_2,\dots$ then $\bigcap C_i$ are the zeroes of $\bigcup P_i$.
    \item Finite unions: If $C_1,C_2$ are the zeroes of $\{f_1,f_2,\dots\}$ and $\{g_1,\dots\}$ respectively, then $C_1\cup C_2$ are the zeroes of $\{{f_i}{g_j}\}$
    \item Clear that $\mathbb{A}^n$ and $\emptyset$ are algebraic sets, taking the zero set of $0$ and of a constant non-zero polynomial respectively.
\end{itemize}

With these properties we see that we can create a topology where the closed sets are the algebraic sets. We call this topology the \textbf{Zariski topology}. 

\begin{example}
    Take ${\mathbb{A}^1}$, this is just the line. The closed sets:\begin{itemize}
        \item $\mathbb{A}^1$, the zero set of $0$.
        \item Any finite sets, since $\{a_1,\dots,a_n\}$ is the zero set of $(x-a_1)\cdots (x-a_n)$
    \end{itemize}

These are the only closed sets, since a polynomial in one variable is either the zero polynomial or has a finite amount of roots.


Take any points $x\neq y$ and nbhs $U$ of $x$ and $V$ of $y$. Since $U^c$ and $V^c$ are finite (note if they are infinite then $U$ or $V$ is the empty set which is impossible). So $U$ and $V$ both contain all the points of $\mathbb{A}^1$
except for a finite number of points. Since $k$ is infinite this means that $U\cap V \neq \emptyset$. This is true for all nbhs of $x$ and $y$, which means this space is not Haussdorff.
\end{example}

\begin{example}
    Take $\mathbb{A}^2$. The closed sets are:\begin{itemize}
        \item Points $(a,b)$
        \item Any curve, the set of zero of $f(x,y) = 0$
        \item Union of finite amount of curves and points
    \end{itemize}

    Note $\mathbb{A}^2\neq \mathbb{A}^1\times \mathbb{A}^1$. Indeed since $\mathbb{A}^1\times \mathbb{A}^1$ are unions of finite amount of vertical lines, horizontal lines and points.
\end{example}

\begin{example}
    \textbf{\textit{Determinantal variety:}}
Take $\mathbb{A}^{mn} = \text{linear maps }k^m\rightarrow k^n = m\times n \text{ matrices}$

Determinantal variety = set all linear maps of rank $\leq i$.

\

We claim that this is an algebraic set. Indeed this set is given by the vanishing of all $(i+1)\times (i+1)$ minors of $m\times n$ matrix. This is a set of polynomials.

In partucular the subset of maps from $k^m\rightarrow k^n$ that are onto is open in the Zariski topology.
\end{example}

\section{Noetherian spaces and Noetherian Rings}

\subsection{Noetherian rings}
\begin{definition}
    A \textbf{Noetherian ring} is a ring satisfying these three equivalent conditions:\begin{itemize}
        \item Every ideal is finitely generated
        \item Every nonempty set of ideals has a maximal element
        \item Every chain of increasing ideals,$I_0\subseteq I_1\subseteq I_2\subseteq \dots$, is eventaully constant. I.e. there is a $n$ such that $I_n=I_m$ for all $m\geq n$. 
    \end{itemize}
\end{definition}
\begin{theorem} \textbf{\textit{Noether}}

    \

    If $R$ is Noetherian then $R[x]$ is Noetherian.
    \begin{proof}
        Let $I$ be an ideal of $R[x]$ and look at the chain $I_0\subseteq I_1\subseteq I_2\subseteq \dots$, of $R$ where:\begin{equation*}
            I_n = \text{leading coeffs of polynomials of degree }\leq n\text{ in I}
        \end{equation*}
Since $R$ is Noetherian this stabilises, so $I_N=I_{N+1}=\dots$, for some $N$.

Take the set of polynomials $s_0,s_1,\dots,s_N$ where:\begin{equation*}
    s_i= \text{ degree }i\text{ polynomials whose leading coefficients generate }I_i, \text{ for }i=1,\dots,N
\end{equation*}
Note the sets $s_i$ are finite since $R$ is Noetherian.


Then $s_0,s_1,\dots,s_N$ generate the ideal $I$.
    \end{proof}
\end{theorem}
\begin{corollary} \textbf{\textit{Hilbert}}

    \

    $k[x_1,\dots,x_n]$ is Noetherian

    \begin{proof}
        Since $k$ is a field any ideal in $k$ is either generated by $0$ or generated by $1$. So $k$ is Noetherian, so $k[x_1]$ is Noetherian.
        So inductively we can see that $k[x_1,\dots,x_n]$ is Noetherian.
    \end{proof}
\end{corollary}

\subsection{Noetherian spaces}

\begin{definition}
    A topological space is called \textbf{Noetherian} if equivalently:\begin{itemize}
        \item The closed sets satisfy the descsending chain condition. So any decreasing sequence:\begin{equation*}
            C_0\supseteq C_1\supseteq C_2\supseteq \dots
        \end{equation*}
        Stabilises, i.e. $C_n=C_{n+1}=\dots$, for some $n$
        \item Any nonempty collection of closed sets has a minimal element.
    \end{itemize}
\end{definition}
These are in some sense the dual of the definition of Noetherian Ring.

\begin{theorem}
    $\mathbb{A}^n$ with Zariski topology is Noetherian.

    \begin{proof}
        Sketch:

        Closed sets of $\mathbb{A}^n$ correspond to some ideals of $k[x_1,\dots,x_n]$. A descsending chains of closed sets of $\mathbb{A}^n$ correspond to an ascending chain of ideals in $k[x_1,\dots,x_n]$
    \end{proof}
\end{theorem}

\

Noetherian spaces are ``WEIRD'', the Noetherian condition is equivalent to saying that every open set is compact or quasicompact. Note quasicompact actually means the same as the regualar defintion of compact, Bourbaki made a mistake in the defintion and 
only considered Haussdorff compact spaces to be compact, so when non-Haussdorff compact spaces were seen to be important they had to use the term quasicompact.

\

\paragraph*{Borcherds' Rule of Thumb} If we see the word ``quasi'' in mathematics then someone, somewhere and somewhen screwed up the terminology and had to use the term ``quasi'' to fix it.

\

\begin{remark}
    In analysis we almost never see open compact sets, it can be shown that if a space is Noetherian and Haussdorff then it is finite.
\end{remark}

\subsection{A first definition of Algebraic Varieties}

\subsubsection{Irreducible sets}

\begin{definition}
    A set is called \textbf{irreducible} if and only if it is nonempty and not the union of $2$ proper closed subsets.
\end{definition}

\begin{definition}
    \textbf{Noetherian Induction}, pick a maximal closed set of some collection of closed sets.
\end{definition}

\begin{theorem}
    Any Noetherian space is a finite union of irreducible subspaces.

    \begin{proof}
        Proof by Noetherian Induction:

        We will show that every closed subset is a finite union of irreducibles. If not, pick a minimal counterexample $C$. We have two cases\begin{enumerate}
            \item If $C$ is irreducible, then we are done and we have found a contradiction
            \item If $C$ is not irreducible, then we can write $C=C_1\cup C_2$, where $C_1,C_2$ are smaller. By induction, $C_1,C_2$ are a finite union of irreducible sets, and so is $C$. Which is a contradiction.
        \end{enumerate}

        In all cases we have a contradiction so, there can't be a closed set that is not a finite union of irreducibles. So all closed sets, in particular the whole space is a finite union of irreducibles.
    \end{proof}
\end{theorem}

So we can reduce the study of Noetherian spaces to the study of irreducible Noetherian spaces.


\

\begin{corollary}
    Every algebraic set is a finite union of irreducible algebraic sets.
\end{corollary}

\begin{definition}
    \textbf{A provisional definition of Algebraic Varieties}:
    
    They are irreducible closed subset of affine space.
\end{definition}

\subsubsection{Examples}

\begin{example}
  This definition is not perfect. Suppose we take the variety given by: $xy=1$ and the set of nonzero points in $\mathbb{A}^1$.

Note the set of nonzero points in $\mathbb{A}^1$ is not a closed set, but since we can map the hyperbola to this by the mapping $(x,y)\rightarrow x$, we should consider it an algebraic variety.

\

\href{https://youtu.be/D_eJ8BWLb24?t=1110}{\includegraphics[scale = 0.25]{algebraic-varieties.png}}

\

We shall give a better definition later (I guess we can clook at these like ``quasi-algebraic varieties'').
\end{example}

\begin{example}
    Take the algebraic set defined by $x^2+y^2 - 2z^2 = 0 \text{ and }2x^2-y^2-z^2 = 0$.

    This is the union of four irreducible subsets:\begin{enumerate}
        \item $x=y=z$
        \item $x=0y=z$
        \item $x=-y=-z$
        \item $x=y=-z$
    \end{enumerate}

    So the intersection of irreducible sets may not be irreducible.
\end{example}
\newpage 
\begin{example}
    If we take $xy=0$, we have the union of the $x$-axis and the $y$-axis.

    \

    If we take $xy=1$, the we hyperbola, which is irreducible and connected in the Zariski topology, but disconnected in the usual topology.

    \

\href{https://youtu.be/D_eJ8BWLb24?t=1295}{\includegraphics[scale=0.25]{connected-irreducible-spaces}}

\end{example}

We have now conclueded the section on Noetherian spaces, next section we will look at the Hilbert Nullstellensatz, and the connection between Algebraic Varieties and Ideals.


\section{Hilbert's Nullstellensatz}
\begin{definition}
    \textbf{Hilbert's Nullstellensatz}(zero's theorem) describes the relation between Ideals in a polynomial ring and Algebraic subsetes of the corresponding affine sets.

    \begin{itemize}
        \item If we have a subset $Y\subseteq \mathbb{A}^n$ we can map it to the ideal $I(Y)\subseteq k[x_1,\dots,x_n]$ where $I(Y)$ is the set of polynomials vanishing on $Y$.
        \item If we have an ideal $\mathfrak{a}\subseteq k[x_1,\dots,x_n]$ we can map it top the set $Z(\mathfrak{a})$ which is the set of zeros of all the polynomials of $\mathfrak{a}$.
    \end{itemize}
\end{definition}
\paragraph*{What is the relationship between these two operations?}
\begin{align*}
    Z(I(Y)) = \text{closure of }Y \text{ in the Zariski topology}
\end{align*}

This fact basically comes from the definiton of the Zariski topology, let $W$ be any closed set contianing $Y$. We know that $W$ is the zero set of some set of polynomials $S$. For all $f\in S$ we have:\begin{equation*}
    f(x) = 0\text{ for all }x\in Y
\end{equation*}
Therefore $S\subseteq I(Y)$, therefore for all $f\in S$ and we have by definition of $Z(\cdot)$:\begin{equation*}
    f(y) = 0 \text{ for all }y\in Z(I(Y))
\end{equation*}

So $y\in Z(S)$, therefore $Z(I(Y))\subseteq W = Z(S)$. Since $Z(I(Y))$ is the smallest closed set containing $Y$ we indeed see that it is the closure.

\

On the other hand is it true that $I(Z(\mathfrak{a})) = \mathfrak{a}$ for all ideal $\mathfrak{a}\subseteq k[x_1,\dots,x_n]$? \textbf{NO}:\begin{example}
    Let $\mathfrak{a} = (x^2)\subseteq k[x]$, and $Z(\mathfrak{a}) = 0$ so $I(Z(\mathfrak{a})) = I(0) = (x)$.
\end{example}

More generally, if $f^n\in \mathfrak{a}$, then $f\in I(Z(\mathfrak{a}))$, since certainly $f$ vanishes at all points of $Z(\mathfrak{a})$ since $f^n$ does.

Therefore \begin{equation*}
    \{f\in k[x_1,\dots,x_n]\mid f^n\in \mathfrak{a}, \text{ for some }n\in\N^\ast\} = \sqrt{\mathfrak{a}} \subseteq I(Z(\mathfrak{a}))
\end{equation*}

Note $\sqrt{\mathfrak{a}}$ is an ideal.


\paragraph*{Is $\sqrt{a} = I(Z(\mathfrak{a}))$?}

\textbf{NO!}

\begin{example}
    If $\mathfrak{a} = (x^2+1)\in \R[x]$, then $Z(\mathfrak{a}) = \emptyset$, and $I(Z(\mathfrak{a})) = \R[x] \neq \sqrt{(x^2+1)} = (x^2+1)$
\end{example}

\subsection{Weak Nullstellensatz}
\paragraph*{What are the maximal ideals of a polynomial ring $k[x_1m\dots,x_n]$}

There are Obvious maximal ideals:

Let $(a_1,\dots,a_n)\in \mathbb{A}^n$ and take the ideal $(x-a_1,x-a_2,\dots,x-a_n) = $functions vanishing on $(a_1,\dots,a_n)$.

\

Note that \begin{equation*}
    k[x_1,\dots,x_n]/(x-a_1,x-a_2,\dots,x-a_n) \simeq k
\end{equation*}

So this is a maximal ideal. Are all ideals of this form? Not in general, if we take $\R[x]$ then $(x^2+1)$ is a maximal ideal, since $\R[x]/(x^2+1) \simeq \C$.

But this is true if $k$ is \textbf{Algebraically closed!} This is the weak Nullstellensatz.

\begin{theorem} \textbf{Weak Nullstellensatz}
For $k$ algebraically closed all maximal ideals of $k[x_1\dots,x_n]$ are of the form $(x_1-a_1,\dots,x_n-a_n)$.
    \begin{proof}
        \begin{lemma}
            If $K$ is a field an is a finitely generated algebra over $k$, then $K$ is a finitely generated module over $k$.
            \begin{proof}
                We are going to cheat: We will assume that $k$ is uncountable. (If $k$ is countable the proof is harder).

                Since $K$ is a finitely generated algebra, then $K$ is at most countable dimension as a module. If $x\in K$ is transcendental over $k$, if we had, for some $a_1,\dots,a_n\in k$ such that  $\sum_{k=1}^n \frac{a_k}{x-a_k} = 0 \Rightarrow \sum_{k=1}^n a_k\prod_{i\neq k}(x-a_j) = 0$, which is impossible since $x$ is transcendental.
                
                \
                
                So the elements $\frac{1}{x-a}$ for $a\in k$ form an uncountable linearly independant set, which is a contradiction to the fact that $K$ is a finitely generated algebra over $k$.

                \

                So all $x\in K$ are algebraic over $k$, therefore $K$ is finitely generated as a module over $k$.
            \end{proof}
        \end{lemma}
        Suppose that $I$ is a maximal ideal of $k[x_1,\dots,x_n]$ we want to show that $I = (x-a_1,\dots,x-a_n)$ for some $(a_1,a_2,\dots)$.

        Let $K = k[x_1,\dots,x_n]/I$, then $K$ is a field and is finitely generated as an algebra. So by the previous lemma $K$ is finietely generated as a module. In other words $K$ is algebraic over $k$,

        But since we assumed that $k$ is algebraically closed we have $k=K$.

        Therefore, $x_i+I\in k$ for all $i$ so $x_i+I=a_i+I$ for some $a_i\in k$ so $x_i-a_i\in I$. So $(x_1-a_1,\dots,x_n-a_n)\subseteq I$ so $I = (x_1-a_1,\dots,x_n-a_n)$, since $(x_1-a_1,\dots,x_n-a_n)$ is a maximal ideal and $I\not=k[x_1,\dots,x_n]$.
    \end{proof}
\end{theorem}

\subsection{Strong Nullstellensatz}

\paragraph*{Proof of SN with Rabinovitsch trick}

\begin{theorem}\textbf{Strong Nullstellensatz}
    If $k$ is algebraically closed then for all ideals $\mathfrak{a}\subseteq k[x_1,\dots,x_n]$ we have $I(Z(\mathfrak{a})) = \sqrt{a}$.

\begin{proof}
    
Suppose that $\mathfrak{a}$ is generated by elements $f_1,\dots,f_n$ and that $f\in I(Z(\mathfrak{a}))$, we want to show that $f\in\sqrt{\mathfrak{a}}$.

\ 

\textbf{Rabinovitsch idea} is too add an extra variable $x_0$.

\

So $f_1,\dots,f_m,1-x_0f$ have no common zeroes in $\mathfrak{A}^{n+1}$, since if $x$ is a zero of $f_1,\dots,f_n$ then it is a zero of $f$, so $1-x_0f(x) = 1$.

Now apply the weak Nullstellensatz in $\mathbf{A}^{n+1}$, since $f_1,\dots,f_m,1-x_0f$ have no common zeroes, they are not contained in any of the maximal ideals(Since if they are contained in a maximal ideal $(x_1-a_1,\dots,x_n-a_n)$ then $(a_1,\dots,a-n)$ would be a common zero) so they generate the unit ideal.

So there exists $g_i\in k[x_0,x_1,\dots,x_n]$ such that $1 = a_0(1-x_0f)+g_1f_1+\dots+{g_n}{f_n}$

Let $x_0 = \frac{1}{f}$, (we are now working in the ring of rational function) we have:\begin{equation*}
    1 = g_1\bigg(x_1,\dots,x_n,\frac{1}{f}\bigg)f_1+g_2\bigg(x_1,\dots,x_n,\frac{1}{f}\bigg)f_2+\dots+g_n\bigg(x_1,\dots,x_n,\frac{1}{f}\bigg)f_n
\end{equation*}
We can clear denominators by multiplying by a high power of $f$ and we get:\begin{equation*}
    f^n = \sum h_if_i \text{ where }h_i = {f^n}{g_i}\in k[x_1,\dots,x_n]
\end{equation*}
So $f^n\in (f_1.\dots,f_n)$ so $f\in \sqrt{(f_1.\dots,f_n)} = \sqrt{\mathfrak{a}}$.
\end{proof}
\end{theorem}
 \begin{tabular}{|c|c|c|}
    \hline
    Affine space $\mathbb{A}^n$& correspondence & $k[x_1,\dots,x_n]$\\
    \hline
    Points $(a_1,\dots,a_n)$ & $\underbrace{\iff}_{Weak Nullstellensatz}$ & maximal ideals $(x_1,\dots,x_n)$ \\
    \hline
    Algebraic sets  & $\underbrace{\iff}_{Strong Nullstellensatz}$  & Radical ideals $\mathfrak{a} = \sqrt{\mathfrak{a}}$\\
    \hline 
    closed subschemes &$\underbrace{\iff}_{\textit{Will see in second part of course}}$ & All Ideals \\
    \hline
\end{tabular}


\begin{example}

    In this example we have a curve described by the radical ideal $(y-x^2)$ and one given by the radical ideal $(y)$.

    We look at the intersection between the two curves by looking at the zero set of the ideal $(y-x^2,y) = (y,x^2)$. However, this ideal is not a radical ideal, as the root of this ideal is $(x,y)$ which corresponds to the point $(0,0)$ in affine space.

    \

    \href{https://youtu.be/1UvW5iTkbLw?t=772}{\includegraphics[scale = 0.25]{Strong_Null.png}}

    \

    So we have a non-radical ideal that is the intersection of two curves. This ideal is non-radical since if we look at the intersection between the two curves we should be counting the intersection point as two points.
\end{example}

\begin{example}\textit{Nilpotent matrices}

    \begin{definition}
        A matrix $A\in M_n(k)\simeq\mathbb{A}^{n^2}$ is \textbf{Nilpotent} if $A^n = 0$ for some $n\in \N$,
    \end{definition}
 
Notice that if:\begin{equation*}
    A = \begin{bmatrix}
        a_{11} & a_{12} & \dots\\
        a_{21}\\
        \vdots
    \end{bmatrix} \Rightarrow A^n = \begin{bmatrix}
        \text{ something complicated, homogenous polynomials in degree }n\text{ in coeffs }a_{ij}
    \end{bmatrix}
\end{equation*}

The entries of the matrix $A^n$ generate an ideal $I$ in the coordinate ring of $A^{n^2}$, in some sense this ideal describes the set of nilpotent matrices. Is $I = \sqrt{I}$? The answer is NO!

If $A$ is nilpotent, then all eigenvalues are $0$, so the trace is $0$. So $a_{11}+a_{22}+\dots+a_{nn}\not\in I$, but this element is in $\sqrt{I}$ by Hilbert's Nullstellensatz.

\

Let us take $n=1$, let $A = \begin{bmatrix}
    a & b\\
    c & d
\end{bmatrix}$ such that $A^2 = 0$. But $A^2 = \begin{bmatrix}
    a^2+bc & b(a+d)\\
    c(a+d) & d^2+bc
\end{bmatrix}$

So $I = (a^2+bc, b(a+d), c(a+d), d^2+bc)$. We have seen that some power of $\text{tr}(A) = a+d$ is in $I$. What is the smallest power?
An easy assumption is that $(a+d)$ is in $I$, but this is false! The smallest power of $(a+d)$ in $I$ is given for $(a+d)^3$.

\begin{equation*}
    (a+d)^2 = a^2 + 2ad + d^2
\end{equation*}

\begin{equation}
    (a+d)^3 = a^3+3a^2d+3ad^2+d^3
\end{equation}

\end{example}

\paragraph*{What about communting matrices?}
\begin{example}
    Let $AB=BA$, with $A=(a_{ij}),B=(b_{ij})\in M_n(k)$.

    Let $I$ be the ideal generated by the entries in $AB-BA$, is $I = \sqrt{I}$?

    We don't know this is a hard open problem.
\end{example}
\lecture{}

\section{The Lasker Noether Theorem}

\subsection{Some background}

Let us recall that Algebraic sets correspond to radical ideals. Since algebraic sets correspond to finite unions of irreducible algebraic sets, and these sets corresponds to Prime ideals, from this we can conclude the following theorem.

\begin{theorem}
    Any radical ideal is the intersection of finite number of prime ideals.
\end{theorem}

\paragraph*{Question: What about ideals that are not radical? Is there a similar decomposition?}

The answer is given to us by Lasker: Ideals of $k[x_1.\dots,x_n]$ is the intersection of a finite number of primary ideals.

\

Noether generalised this theorem by showing that it is true for Noetherian rings.

\begin{definition}\textbf{Associated Prime}
    Let $M$ be a module over $R$, and \textbf{associated prime }$\mathfrak{p}$ is a prime ideal $\mathfrak{p}$ such that M contains a submodule isomomorphic to $R/\mathfrak{p}$, for brevity we write $R/\mathfrak{p}\subseteq M$.
\end{definition}

\begin{definition}\textbf{Coprimary Module}
    We have two definitions:\begin{itemize}
        \item A module $M$ over $R$ is called \textbf{coprimary} if:
        \begin{equation*} ab = 0, \text{ for }a\in R\text{ and }b\in M, \Rightarrow b=0\text{ or }a^n=0\text{ for some n} \tag{$\dagger$}\end{equation*}
        \item\textit{A more convinent definition} A module $M$ over $R$ is coprimary if it has exactly one associated prime $\mathfrak{p}$.  
    \end{itemize}
\end{definition}

\begin{remark}
    If the ring $R$ is Noetherian, the two definitions of coprimary are equivalent for finitely generated modules.
\end{remark}

\begin{definition}\textbf{Primary ideal}
\begin{itemize}
    \item  \textit{A first definition by Lasker}: $\mathfrak{p}\subseteq R$ is primary if and only if $ab\in \mathfrak{p}$ implies $a\in \mathfrak{p}$ or $b^n\in \mathfrak{p}$ for some $n$.
    \begin{example}
        In $k[x]$: $(x)$ is prime and $(x^n)$ is primary. Note in general primary ideals need not be powers of prime ideals
    \end{example}

    \item \textit{Another definition}: An ideal $\mathfrak{p}\subseteq R$ is primary if and only if the module $R/\mathfrak{p}$ is coprimary
\end{itemize}
\begin{remark}
    In general for a module $M$ over $R$, $N\subseteq M$ is primary $\iff$ $M/N$ is coprimary.
\end{remark}
There are two different definitions for this primary ideal under the other definition but they are equivalent if $R$ is Noetherian.       
\end{definition}

\begin{theorem}\textit{\textbf{Lasker-Noether Theorem for finitely generated modules over Noetherian Rings}}

    Let $M$ be a f.g. module over a Noetherian Ring. Then $0$ an intersection of primary submodules of $M$. 

    Alternatively: $M\subseteq \oplus_{\text{finite}} \text{coprimary modules}$
\end{theorem}
\begin{example}
    For $\Z$-modules (abelian groups): Some example of coprimary modules:\begin{itemize}
        \item $\Z^n$ where the prime ideal is $(0)$
        \item Any finite group of order $p^n$ where the prime ideal is $(p)$
    \end{itemize}
    So our theorem says that a finitely generated abelian group is contained in direct sum of copies of $\Z$ and finite groups of prime power order. If we recall the structure theorem this inclusion is actually an equality.        
\end{example}

\begin{proof}\textit{Lasker-Noether Theorem}
    
    This proof has two steps:\begin{enumerate}
        \item \begin{lemma}
            Any submodule $M$ is an intersection of a finite number of irreducible submodules.

            \begin{proof}
                Noetherian Induction. What does this mean? The submodules of $M$ have the property that any non-empty subset has a maximal element (since $M$ is f.g.  over a Noetherian ring).
            So suppose $N$ is a maximal submodule that is not a finite intersection of irreducibles, so either this submodule is irreducible which is a contradiction since it is an interesection of itself, or it is the intersection of two larger submodules which are by induction a finite intersection of irreducibles, contradiction.
            \end{proof}
        \end{lemma}
        \item \begin{lemma}
            Any irreducible submodule $N$ is primary
        \end{lemma}
        \begin{proof}
            We can reduce to the case where $N$ is $0$ by taking quotients. So we want to show that $(0)$ is irreducible implies that $M$ is coprimary.

            But if $M$ is not coprimary, then there are two associated primes $\mathfrak{p}$, $\mathfrak{q}$ such that $M$ has submodules isomorphic to $R/\mathfrak{p}$ and $R/\mathfrak{q}$.

            These submodules have intersection $0$ which implies that $0$ is not irreducibles.
        \end{proof}
    \end{enumerate}

    Combining the two previous lemmas gives us our proof.
\end{proof}
\begin{example}
    Let us look at the ideal generated by $(xy,y^2)$ in $k[x,y]$.

    The algebraic set is given by the intersection of $V(xy) = x\text{ and }y-axis$ and $V(y^2) = x$-axis (doubled up in some informal sense).

    The intersection of these two sets is just the $x$-axis, whith a sort of ``double point'' near the origin.

    We can understand this ``double point'' by looking at the primary decomposition of the ideal:
    \[(xy,y^2) = (y)\cap \underbrace{(x.y^2)}_{\text{embedded component}}\]

    \href{https://youtu.be/1oKh8QJ1I4k?t=619}{\includegraphics[scale = 0.25]{lasker_noether.png}}

Note this primary decomposition is not unique:\begin{align*}
    (xy,y^2) &= (y)\cap (x,y^2)\\
             &= (y)\cap (x+y,y^2)
\end{align*}
\end{example}

\section{Quotients of varieties by groups}

\subsection{Coordinate rings}
Recall for any algebraic set, $Y$ we can go to it's coordinates ring $k[x_1,\dots,x_n]/I(Y)$, this can be thought of as the ring of all polynomials on the algebraic set $Y$.
Because we are basically ``setting to zero'' all the polynomials which vanish on $Y$, so any two polynomials that are equal on $Y$ are considered the same. So we can find an isomorphism between the ring of polynomials on $Y$ and this coordinate ring by sending $f+I(Y)\rightarrow f|_Y$. This is well definied since if $g+I(Y) = f+I(Y)$, then since $g-f\in I(Y)$, $g(\bar{x}) = f(\bar{x})$ for $\bar{x}\in Y \Rightarrow f|_Y = g|_Y$.

This is an injection since if $f|_Y = g|_Y$, then $g(\bar{x}) = f(\bar{x})$ for $\bar{x}\in Y$, so $f-g\in I(Y)\Rightarrow f+I(Y) = g+I(Y)$.

Finally let $\omega$ be any polynomial on $Y$, then $\omega(x_1,\dots,x_n) = \sum_{i} {a_i}{m_i(x_1,\dots,x_n)}$, for some monomial $m_i$. So we just need to set $f = \sum_{i} {a_i}{m_i(x_1,\dots,x_n)}\in k[x_1,\dots,x_n]$.

\

This coordinate ring has three properties:\begin{enumerate}
    \item Algebraic over $k$
    \item finitely generated
    \item No nilpotent elements
\end{enumerate}

On the other hand, if we have an algebra with these three properites, then by strong Nullstellensatz it corresponds to an algebraic set which is not unique, since it depends on the generators chosen. But they are isomorphic in some sense.


Informally algebraic sets (up to isomorphism) as being more or less equivalent to algebras with the above properites. In technical terms the category of algebraic sets is equiv to opposit of category of coordinate rings.

\subsection{Application}
Suppose we have an algebraic set $Y$ acted on by a group $G$. Can we form the quotient $Y/G$? This doesn't really work. One of the problems is how do we embed the quotient in affine space? But if we look at coordinate rings this is easier to do, since the functions on $Y/G$ should correspond to the functions on $Y$ invariant under $G$.
\begin{center}
    So we can look at the \textbf{Ring of invariant} ${\bigg(k[x_1,\dots,x_n]/I(Y)\bigg)}^G$.     
\end{center}
So not this is an algebra over $k$ and has no nilpotent elements, but is it finitely generated? The answer is sometimes, but Hilbert proved that in many cases it is finitely generated but some examples of when it isn't finietely generated were found after him.


\begin{example}
    Let $\mathbb{A}^n =$ affine space, and $G  = S_n$ permutations of coordinates. The polynomial ring is ${k[x_1,\dots,x_n]}^{S_n}$ which is generated by the \textbf{elementary invariant polynomial}:\begin{itemize}
        \item $p_1 = x_1+\cdots + x_n$
        \item $p_2 = x_1x_2+\cdots$
        \item $\vdots$
        \item $p_n = x_1\dots x_n$
    \end{itemize}

    So \[\mathbb{A}^n/S_n \simeq \mathbb{A}^n\]

    While this example is easy, in general quotients of affine space by groups are complicated. Most of times we get a nice answer when the group is a reflection group (like $S_n$).
\end{example}

\begin{remark}
    We have to be careful, the quotient might not be what we expect. If we take the real line and have the group $\Z/2\Z$ acting by $x\rightarrow -x$, we might think that the quotient is the half closed interval (which us what we would get if we were identifying points) it isn't the same as what we get in our construction above.

    We get the whole real line back. The positive points are given by identifying $(2,-2)$ but the points in the negative side are given by identifying $(2i,-2i)$, these points look like complex points but they count as real points in the quotient.
\end{remark}

\begin{example}
    Let us look at:\begin{equation}
        GL_n(k)\text{ acting on }k^n \Rightarrow \text{The orbits are }0, \text{ or everything else}
    \end{equation}

    So we might think that the quotient would give us two points, but instead we only have one point since the only invariant polynomials acted on by $GL_n(k)$ are constants. Since we have for all $A\in GL_n(k):$ \[f(A(x_1,\dots,x_n)) = f(x_1,\dots,x_n)\text{ for all }(x_1,\dots,x_n)\in k^n\] 


Then pick any non-zero points $(x_1,\dots,x_n); (y_1,\dots,y_n)$ and we see that $f(x_1,\dots,x_n) = f(A(x_1,\dots,x_n)) = f(y_1.\dots,y_n)$ for some $A\in GL_n(k)$. Since this polynomial is the same at an infinite amount of points, it is the constant polynomial.

So the only invariant polynomials are the constant polynomials, this ring is isomorphic to $k$ which is the coordinate ring of a point.
\end{example}

\begin{example}\textbf{Classical invariant theory}
Let $G = SL_2(\C)$, it acts on ${a_n}x^n + {a_{n-1}}x^{n-1}y+\cdots + a_0y^n$, by:\[\begin{bmatrix}
    a & b\\c&d
\end{bmatrix}\begin{bmatrix}
    x\\y
\end{bmatrix} = \begin{bmatrix}
    ax+by\\cx+dy
\end{bmatrix}\]

So $(a_0,\dots,a_n)\in \mathbb{A}^n$ we can ask what is $\mathbb{A}^{n+1}/SL_2(\C)$? The coordinate ring are all polynomials in $(a_n,a_{n-1},\dots)$ which are invariant under $SL_2(\C)$. These are called invariants and finding them is part of classical invariant theory.

For example $a_2x^2+a_1xy+a_0y^2$, the invariant is the \textbf{discriminant} which is $a_1^2-4({a_0}{a_2})$

\

Is the ring of invariants finitely generated? Paul Gordan (``king of invariant theory'') proved that this was true, but Hilbert gave us an shorter way of proving this, the \textbf{Hilbert finiteness theorem}. 

\end{example}

\subsection{Hilbert's fintieness theorem}

\begin{definition}
    Let $A = k[x_1,\dots,x_m]$ and $G$ be a group acting on $k^n =\text{span}\{x_1,\dots,x_m\}$ so $G$ acts on $A$. Let $A^G = $invariant elements of $A$ and assume that $G$ is finite and $\text{char} k = 0$.
We define the \textbf{Reynolds operator} to be $\rho$, where $\rho(a) = \text{average of }A \text{ under G}$. So for $G$ finite we have\begin{equation}
    \rho(a) = \frac{1}{|G|}\sum_{g\in G}g(a)
\end{equation}
Note we needed the assumptions in our statement to define this number (the sum is defined since $G$ is finite and we can define the average since characteristic of $k$ is zero, so we are not dividing by zero).

This operator has the following properties:\begin{itemize}
\item $\rho(1) = 1$
\item $\rho(a+b) = \rho(a)+\rho(b)$
\item In general $\rho(ab)\neq \rho(a)\rho(b)$, this is not a homomorphism of algebras
\item $\rho(ab) = a\rho(b) = \rho(a)\rho(b), \textit{ if }a=\rho(a)$
\end{itemize}

So $\rho$ is a homomorphism from $A$ to $A^G$ of $A^G$ modules.
\end{definition}
\begin{theorem}
    Let $A = k[x_1,\dots,x_m]$ and $G$ be a group acting on $k^n =\text{span}\{x_1,\dots,x_m\}$ so $G$ acts on $A$. Let $A^G = $invariant elements of $A$.
    $A^G$ is finitely generated as a $k$-algebra if $G$ is finite and $\text{char} k = 0$.
    
    \begin{proof}
        Notice that $A$ is graded by degree:\begin{eqnarray}
            A = A_0\oplus A_1\oplus A_2\dots
        \end{eqnarray}
        Where $A_0 = k$, $x_1,\dots,x_n\in A_1$, etc.,.. This is the usual way of grading a polynomial ring.

        Let $I = $ideal of $A$ generated by the homogenous elements of $A^G$ of degree greater than zero. Notice that $I$ is finitely generated as an ideal, we can assume that it has a finite number of generators in $A^G$ and are homogenous.

        \

        Suppose $i_1,\dots,i_n$ are generators as above for the ideal $I$, we want to sho they genereate the $k$-algebra $A^G$. 

        We show by induction of the degree that if $x\in A^G$ and $x$ is homogenous, then $x$ is in the algebra generated by $i_1,\dots,i_n$. 
        
        This is obvious for degree $0$, so assume that $\deg x>0$. Let $x=a_1i_1+\cdots+{a_k}{i_k}$, for some $a_i\in A$ homogenous. So $x$ is in the ideal generated by $i_1,\ldots,i_n$ but $a_i$ NEED NOT be in $A^G$.

        Applying the Reynolds operator:\[x \underbrace{=}_{x\in A^G} \rho(x) = \rho(a_1)i_1+\rho(a_2)i_2+\ldots\] 
        
        Since $\rho(i_m) = i_m$ (since $i_m\in A^G$).
        


        By properties of the Reynold operator $\rho(a_i)\in A^G$, therefore $x$ is a polynomial in elements of $A^G$ of smaller degree. So $\deg \rho(a_i)<\deg x$ so by induction $a_i$ are polynomials in $i_m$ and so $x$ is also.
    \end{proof}
\end{theorem}

Hilbert didn't just prove this for finite groups but also for more general groups.

\paragraph*{Extensions of this thorem}\begin{enumerate}
    \item For $G$ compact and $k=\R$ or $\C$, the same proof works since we can define a Reynods operator:$\rho(a) = \frac{1}{\text{vol}G}\int_Gg(a)$
    \item $SL_n(\C)$? We use Weyl's unitarian trick. Since $SL_n(\C)\supseteq \underbrace{SU_n(\C)}_{\text{compact}}$. Complex actions of $SL_n(\C)$ on complex vector space $V$ (finite dimension) are the ``same'' as actions of $SU_n(\C)$ on $V$.
\end{enumerate}


\paragraph*{Nagata's Example}
This is an example of group $G$ acting on $k^n$ so that $A^G$ is not finietely general.

First take group $k\simeq \begin{bmatrix}
    1 & \ast\\
    0 & 1
\end{bmatrix}$ acts on $k^2$, (this is a sort of universal counter example to everything called the \textbf{Unipotent action}, where all eigenvalues of all matrices are $1$).

Then take sixteen copies of this so $k^{16}$ acts on $k^{32}$, and $G =$ ``generic'' $13$-dim subspaces of $k^16$.

\section{Three examples of quotients of algebraic sets}
\subsection{Cyclic quotient singularity}

We take $\mathbb{A}^2$ whose coordinate ring is $k[x,y]$. And $G = \Z/n\Z$ generated by $\sigma$ of order $n$, where $\sigma(x) = \zeta x$ and $\sigma(y) = \zeta y$, where $\zeta$ is a $n$-th root of unity.

Now what are the invariants of the coordinate ring over $G$?

Well notice that \begin{equation}
    \sigma({x^i}{y^j}) = {\zeta}^i{i+j}{x^i}{y^j} = {x^i}{y^j} \text{ if and only if }i+j = 0\mod n
\end{equation}
So the ring of invariants has a basis ${x^i}{y^j}$, where $i+j = 0\pmod n$. This ring is generated by $z_0 = x^3, \ z_1 = x^2y, \ z_3 = xy^2, \ z_4 = y^3$, but these elements are not independant, since ${z_i}{z_j} = {z_k}{z_l}$ when $i+j = k+l$

So in reality the ring of invariant is equal to \[k[z_0,\dots,z_4] / ({z_1}{z_2}={z_0}{z_3}, {z_1}^2 - {z_0}{z_2}, {z_2}^2 - {z_1}{z_2} ) \tag{$\dagger$} \]

So the quotient of the affine plane by the cyclic group is an affine variety whose coordinate ring is the $\dagger$


\subsection{Parameter space of cyclohexane}

\begin{definition}
    A\textbf{ parameter space} is some sort of space whose points corresponds to some ``configurations'', say some sort of algebraic subset of an algebraic variety. Like a parameter space of line inside of space, or parameter space of conics in a plane 
\end{definition}

We are going to look at the parameter space of cyclohexane, recall from chemestry cyclohexane is a ring of six carbon atoms and sixteen hydrogen atoms. 

A carbon atoms centre is specified by three number so a carbon atom corresponds to $\mathbb{A}^3$, so six carbon atoms correspond to $\mathbb{A}^{3\cdot 6} = \mathbb{A}^{18}$. However there are some conditions that this need to satisfy, if one carbon atom is $(x_1,y_1,z_1)$ and another is $(x_2,y_2,z_2)$ they must satisfy:\begin{equation}
    (x_1-x_2)^2+(y_1-y_2)^2+(z_1-z_2)^2 = c \text{ a constant }
\end{equation}

So $6$ quadrics must be satisfied for the distances and $6$ for the angles. So we have $18$-dimension affine space and the points must satisfy $12$ equations. Subce we can translate and rotate our cyclohexane then we can guess that the parameter space is taken by:\begin{equation}
    \text{space of dimension }18-6-6 / \text{group of translations, rotations (six dimension group)}
\end{equation}

What is the dimension? $0$? \textbf{NO} There are two different forms of cyclohexane, the chair form and the boat form. Furthermore the boat form is felxible and we can bend it around to form another boat form.

So parameter space has at least two components, one component that is a point corresponding to the chair form and a one dimensional component corresponding to the boat form of cyclohexane.

So we must be a bit careful when guessing dimension of quotients, the naïve guess is sometimes just wrong.

\subsection{Moduli space}

\begin{definition}
    \textbf{A moduli space} is space whose points corresponds to isomorphism classes of things
\end{definition}
This is similar to a parameter space, it is traditional to use paramater space if we are classifying thins embedding in something else, and moduli space if we are classifying things that aren't really embedded in anything.

\

Now let us look very briefly at the moduli space of elliptic curves.

\begin{definition}
    \textbf{A Elliptic curve} over the complex numbers is a non-singular curve tbat is topologically isomorphoic to a torus. (We will discuss elliptic cutves lates)
\end{definition}

We will later see that elliptic curves can be written as:\begin{align*}
    y^2&= x^3+ax^2+bx+c\\
    y^2&= (x-\alpha)(x-\beta)(x-\gamma)\\
    y^2 &= x(x-\beta)(x-\gamma) \text{ by adding a constant to }x\\
    y^2 &= x(x-1)(x-\lambda) \text{ by rescaling }x, \text{ with }\lambda\neq 0,1
\end{align*}

This is invariant under changing $\lambda \rightarrow \frac{1}{\lambda}, 1-\lambda, \frac{1}{1-\lambda}, \frac{\lambda}{\lambda-1}, \frac{\lambda - 1}{\lambda}$
These six transformations form a group isomorphic to $S_4$.

So we have an affine variety given by \begin{equation}
    (\mathbb{A}^1\setminus\{0,1\})/S_3 \tag{$\dagger$}
\end{equation}

Note $\mathbb{A}^1\setminus\{0,1\}$ is indeed an affine set since it is isomorphic to the curve $\lambda(\lambda-1)\mu = 0$ in $\mathbb{A}^2$ where $\lambda,\mu$ are coordinates.

What is this space $\dagger$? Well if we take the coordinate of this space here which is $k[\lambda, \frac{1}{\lambda}, \frac{1}{\lambda-1}]$ and take a subring that is invariant under the group $S_3$ of order six. In order to describe this we want to find some polynomial in these three coordinates
that is invariant under $S_3$ this is the simplest one:\begin{equation}
    j = \frac{2^8(\lambda^2-\lambda+1)}{\lambda^2(\lambda-1)^2}
\end{equation}

And it turns out that $k[\lambda, \frac{1}{\lambda}, \frac{1}{\lambda-1}] \simeq k[j]$. This $j$ is the so called $j$-invariant of an elliptic curve that we will study more later on.

\end{document}